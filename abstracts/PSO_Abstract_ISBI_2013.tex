\documentclass[9pt,conference,a4paper]{IEEEtran}
\IEEEoverridecommandlockouts

%\usepackage{...}


\title{Particle Swarm Optimization Multitensor Fitting.}
\author{
	\IEEEauthorblockN{
		Michael Paquette\IEEEauthorrefmark{1},
		Eleftherios Garyfallidis\IEEEauthorrefmark{1},
		Samuel St-Jean\IEEEauthorrefmark{1},
		Pierrick Coup\'e\IEEEauthorrefmark{2},
		Maxime Descoteaux\IEEEauthorrefmark{1}
	}

	\IEEEauthorblockA{\IEEEauthorrefmark{1} Universit\'e de Sherbrooke, Sherbrooke, Canada}
	\IEEEauthorblockA{\IEEEauthorrefmark{2} McGill University, Montreal, Canada}

	\thanks{This work is supported by...}
}




\begin{document}
\maketitle

For the purpose of the ISBI HARDI reconstruction challenge 2013 and for the categories DTI and HARDI we reconstructed the data by fitting a multitensor (MT) with particle swarm optimization (PSO) \cite{?pso?}. 

The goal is to find the optimal parameters so that the MT model, $\sum_{i=0}^N f_i e^{-b g^t D_i g}$, fits the measured signal where $D_i$ is a rank 2 symmetric tensor with volume fraction $f_i$, $g$ the normalized gradient wavevector and $b$ the corresponding b-value. This is accomplished by minimizing the squared error $\sum_{k=0}^M \left( \sum_{i=0}^N f_i e^{-b_k g_k^t D_i g_k} - y_k \right) $ for a fixed diffusion signal $y = \{y_k\}_{k=0}^M$, a fixed number of compartement $N$ and a fixed gradient scheme $ \{b_k, g_k\}_{k=0}^M$.

This minimization is perform using the pso. The pso is a stochastic optimization algorithm using population interaction to search the parameter space. We initialize P particules (points in $\mathbf{P}^n$, the size n parameters space of real value) and we move them around according to a velocity $P_z^{(t+1)} = P_z^{(t)} + \phi_1 * u_1 * P_z^{(t)} + \phi_2 * u_2 * P_z^{best} + \phi_3 * u_3 * P_{swarm}^{best}$ where $u_l \sim \mathcal{U}[0,1]$ and $\phi_l$ are user inputed parameters affecting the pso's behavior, $P_z^{t}$ is the z$^{th}$ particule's position at iteration t, $P_z^{best}$ is the z$^{th}$ particule's best known position and $P_{swarm}^{best}$ is the best among the $P_z^{best}$. This equation means that at every iteration, the particules are drawn to swarm's best known position and deflected a bit by their own best location and their last velocity. The particule will try to explore the swarm's best position to find the optimum, while converging there from all over the space, allowing to potentially find new attractor point. All while exploring their own best position which might be a better optimum than the swarm's best. Finally, the random weighting between these quantities and the small "conservation of previous direction" allow the particules the escape local minimum, potentially attracting to them other particules that are stuck.

Considering the given ground truth, a binary connectivity matrix with given ROI, we aimed to validate the quality of our method on something comparable. We computed the same tractography from all the different parameters combination of our local model. We then computed a "connectivity" matrix for the ROIs from the track (track count) and normalized it with the ROI's size (we had seeded only from the ROIs and had a fixed number of seeds per voxel). We then applied different threshold to obtain a binary connectivity matrix and count the direct error ( $\#$ false bundle + $\#$ missing bundle), using that information with several threshold to get a good grasp of the true connectivity strenght (quality?). Indeed, a good tracking should allow a large range of threshold value with low direct error, meaning that, as the threshold grow, false bundle disappear faster than true bundle and as the threshold goes down, missing bundle appear faster than new false bundle.

We did the following test ...

For the final results,

The DW datasets were denoised with the adaptive nonlocal means \cite{manjon-coupe:10} using a rician noise model. As proposed in \cite{descoteaux-wiest-daessle-etal:08}, each DW images were processed independently.

We then launch PSO with with N = 4, with one of those compartement been isotropic. We also fit tensor with $\lambda_2 = \lambda_3$ and $\lambda_1 > \lambda_2$, giving us two eigenvalues and two rotation angles per fiber compartement, one eigenvalue for the isotropic compartement and a volume fraction between the isotropic part and the reste, 14 parameters. We assume that all non-isotropic compartement have equal volume fractions.

From that fit, we extract three peaks. If any voxel has peaks closer to each other than $\theta^\circ$ (e.g. 30$^\circ$), that voxel is re-estimated with one less fiber compartement. This "pruning" provide a good cleaning of the huge overfitting caused because, the peaks tends to converge together when the voxel is been overmodeled, thanks to the isotropic compartement. The only drawback is that we put a hard lower bound on the methods angular resolution ($\theta^\circ$).

After the angular based model reduction, we further fix the overfitting problem (caused by fitting a fixed number of compartement) by looking at the model complexity of neighbooring voxel to prune outlier based on a simple threshold (we re-fit with one less fiber compartement if a voxel is more complex than H\% of it's one-voxel neighboorhood)



%Hello \cite{manjon-coupe:10}, \cite{descoteaux-deriche-etal:09}, \cite{Descoteaux2008}, \cite{tournier-calamante-etal:07}.

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...


...

...

...

...

...

...

...

...

...

...

...

...



...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...

...


\bibliographystyle{ieeetr}
\bibliography{/home/eleftherios/Documents/scil-bibtex/scilBibTex}

\end{document}


